{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Unified Corpus Explorer <p>The Unified Corpus Explorer (UCE) is a generic NLP application developed by the Text Technology Lab that allows the automatic creation of a fully featured portal for your annotated corpora. UCE is standardized in the sense that it's dockerized, reusable, and follows strict schemata\u2014one of which is the UIMA format. To import data and set up your own UCE instance, it is required that the data is annotated or at least exists in UIMA format. </p> <p>This documentation includes step-by-step tutorials for developers as well as users, and shows easy-to-follow instructions. The easiest scenario\u2014one where the data already exists in UIMA format and simply needs to be set up and imported into the UCE instance\u2014can be done with Docker knowledge only.</p> <p>Below you find some running instances of different projects using UCE for their corpora</p>"},{"location":"#running-uce-instances","title":"Running UCE Instances","text":"<p>UCE is used by different projects to visualize their corpora and to provide a generic, but flexible webportal for their users. Here we list some of those UCE instances.</p> Url Project Description URL The TTLab offers a free demo of UCE with multiple corpora, annotations and features aiming to give a general overview of the tool. URL The Specialised Information Service Biodiversity Research (BIOfid) provides access to current and historical biodiversity literature. URL PrismAI A dataset for the systematic detection of AI-generated text, containg both English and German texts from 8 domains, synthesized using state-of-the-art LLMs."},{"location":"publications/","title":"Publications","text":"<p>Here we list all publications related to UCE, which you can also refer to when citing UCE in your work:</p>"},{"location":"publications/#2025","title":"2025","text":"<p>Best Demo Paper Award at NAACL 2025</p> <p>We are delighted that our paper \"Towards Unified, Dynamic, and Annotation-based Visualizations and Exploration of Annotated Big Data Corpora with the Help of Unified Corpus Explorer\" has been awarded the Best Demo Paper at this year\u2019s annual conference of the Nations of the Americas Chapter of the Association for Computational Linguistics (NAACL 2025).</p> <pre><code>@inproceedings{Boenisch:et:al:2025,\n  title     = {Towards Unified, Dynamic and Annotation-based Visualisations and\n               Exploration of Annotated Big Data Corpora with the Help of Unified\n               Corpus Explorer},\n  author    = {Kevin B{\\\"o}nisch and Giuseppe Abrami and Alexander Mehler},\n  booktitle = {2025 Annual Conference of the North American Chapter of the Association\n               for Computational Linguistics -- System Demonstration Track},\n  year      = {2025},\n  keywords  = {uce,biofid},\n  video     = {https://www.youtube.com/watch?v=f3kB9pNPjsk},\n  note      = {accepted, Best Demo Award}\n}\n</code></pre>"},{"location":"about/about/","title":"About","text":"<p>The Unified Corpus Explorer is a novel solution for making UIMA-annotated corpora tangible. Herein, UCE stands as a generic interface that, given any corpus and its extracted UIMA-based annotations, makes the underlying data accessible through various features, including:</p> <ul> <li>Semantic search  </li> <li>Visualization</li> <li>Chatbot Integration  </li> <li>Integration of various UIMA-based annotations  </li> </ul> <p>UCE handles the import of necessary files, sets up a multi-microservice environment, and adapts to the specific needs of each corpus and its annotations. Configuration files enable customization of UCE, including:</p> <ul> <li>Appearance (e.g., color schemes and corporate identity)</li> <li>Selection of active features</li> <li>Integration of annotations</li> </ul> <p>Corpora</p> <p>UCE supports the incorporation of multiple corpora within the same instance.</p>"},{"location":"about/about/#annotate-and-publish-your-data","title":"Annotate and Publish your Data","text":"<p>UCE operates on UIMA-annotated corpora. If the data is already in that format, you can directly import the files through UCE's Corpus Importer.</p> <pre><code>graph LR\n    A[Corpus] ----&gt; B{Annotated?}\n    B ---&gt;|Yes| C[Importer]\n    B ---&gt;|No| D[\u2699 DUUI]\n    C ---&gt; E[\ud83c\udf0d UCE]\n    D ---&gt; C</code></pre> <p>If not, the easiest and most efficient way to transform data into UIMA format while also annotating it is by using the Docker Unified UIMA Interface (DUUI). Refer to our documentation to learn how to utilize DUUI to annotate and transform your corpus before importing it into UCE. Also, check our compatibility list of annotations to see which annotations UCE supports.</p>"},{"location":"about/about/#architecture","title":"Architecture","text":"<p>The Unified Corpus Explorer (UCE) is built on a modular (dockerized) microservice infrastructure. Below is an overview of the architectural flow, described from top to bottom:</p> \ud83d\udce5 Corpus Import and Enrichment <ul> <li>Corpus Importer (A)         Responsible for importing UIMA-annotated documents along with their associated configuration files. Additionally, it interfaces with external services (C and D) to perform enrichment tasks and data preprocessing, ensuring the corpus is optimally structured for downstream processing.     </li> </ul> \ud83e\udde0 AI &amp; NLP Access <ul> <li>Python Web Server (D)         Facilitates the generation of high-dimensional embedding spaces across multiple levels of representation. These embeddings are subsequently employed by other modules, including the integrated Retrieval-Augmented Generation (RAGBot) pipeline. The web server serves as a central access point for incorporating Natural Language Processing (NLP) and Artificial Intelligence (AI) components within UCE.     </li> </ul> \ud83d\uddc4\ufe0f Data Storage <ul> <li>PostgreSQL Database (B)         Provides persistent storage for fully processed and semantically enriched documents. In addition to traditional relational storage, it includes vector capabilities to support similarity-based retrieval using embedding vectors.     </li> <li>SPARQL Endpoint (C)         Supports the storage and querying of ontologies encoded in RDF or OWL formats. These ontologies serve as semantic backbones that enhance search precision and enable structured annotation within UCE.     </li> </ul> \ud83c\udf10 User Access <ul> <li>Web Portal (E)         Acts as the primary interface for users to interact with the system. It provides access to various functionalities, each of which is connected to backend microservices through color-coded connections, indicating underlying service dependencies and architectural integration.     </li> </ul> \ud83e\uddec Feature-Service Interactions <p>Each feature within UCE depends on a specific combination of annotations and backend services, denoted by shorthand references and color-coded wire connections depicted in the system architecture legend. Examples include:</p> <ul> <li>Embedding Search         Relies on services B (PostgreSQL), C (SPARQL), and D (Python Web Server) to function effectively.     </li> <li>Document Reader         Operates solely with service B (PostgreSQL) for retrieving and displaying annotated content.     </li> </ul> <p>This architectural overview provides a generalized representation of the system as it currently exists. Given the ongoing development of UCE, individual components, services, or interactions may evolve over time. The information presented here should be interpreted as a high-level conceptual framework rather than a static specification.     </p>"},{"location":"about/about/#features","title":"Features","text":"<p>Based on the outlined architecture, UCE provides the following features:</p> <ul> <li> <p> Document Reader</p> <p>Provides an interface for accessing and reading all documents within the corpus. Annotations are systematically highlighted to support structured analysis and interpretation. The reader accommodates multiple formats and facilitates efficient navigation across annotated segments.</p> </li> <li> <p> Fulltext Search</p> <p>Enables comprehensive full-text search across the entire corpus. An advanced mode supports Boolean operators such as <code>AND</code>, <code>OR</code>, and <code>NOT</code>, allowing for the formulation of complex and precise search queries. Search functionality is further enhanced through the integration of annotations and contextual filters.</p> </li> <li> <p> Ontology Enrichment</p> <p>Supports the integration of ontologies in RDF or OWL formats to enable semantically enriched search operations. Hierarchical structures, such as taxonomic or legal classifications, facilitate broader and more inferential retrieval strategies, contributing to the semantic understanding of the corpus.</p> </li> <li> <p> Semantic Role Labeling</p> <p>Incorporates Semantic Role Labeling (SRL) annotations to model predicate-argument structures within the text. This facilitates the formulation of semantically grounded queries by addressing the underlying question:</p> <p>Who performed which action, upon whom, and under what circumstances?</p> <p>SRL enhances analytical depth by representing event structures and participant roles explicitly.</p> </li> <li> <p> Embedding Search</p> <p>Facilitates semantic search by projecting documents into a high-dimensional vector space using embedding representations. This method supports cross-lingual and approximate semantic retrieval, making it suitable for fuzzy matching, multilingual corpora, and thematically clustered content exploration.</p> </li> <li> <p> Chatbot (RAGBot)</p> <p>Employs Retrieval-Augmented Generation (RAG) techniques to enable interactive querying of the corpus via a chatbot interface. Following preprocessing, users may engage in natural language dialogue with the system, which retrieves relevant content passages and generates contextually informed responses. This supports exploratory research and information retrieval.</p> </li> <li> <p> Visualization</p> <p>Offers a suite of visualization tools for structural and semantic analysis of the corpus. Available modalities include 2D and 3D graph-based representations, timeline visualizations, and clustering layouts. These visualizations enable the identification of patterns, anomalies, and structural relationships within large corpora.</p> </li> <li> <p> Wikidtion</p> <p>Automatically generates an interlinked hypertext knowledge base derived from the corpus. Each annotation and document is transformed into a dedicated wiki-style entry, promoting interpretability and contextual understanding through semantic linking. This facilitates knowledge integration and conceptual navigation akin to a domain-specific Wikipedia.</p> </li> <li> <p> Authentication</p> <p>UCE is compatible with Keycloak, providing full identity and access management support for authentication. You can define permission systems and user roles, configure login requirements, and create or manage users through the admin cockpit.</p> </li> <li> <p> S3Storage</p> <p>Integrate S3 storage to allow users to download any document imported into UCE, supporting the FAIR principles required by many research and open-source projects.</p> </li> <li> <p> Analysis Engine</p> <p>On-demand analysis and annotation of new documents or free text via the API to the Docker Unified UIMA Interface, enabling real-time NLP tasks such as emotion or sentiment analysis.</p> </li> </ul> <p>Depending on the user configuration and the annotations of the corpora, these features are flexibly enabled or disabled. The most minimal configuration for UCE is to import a corpus without any annotations and disable all processing and features, leaving only document reading and basic search functionality.</p> <p>Have a look yourself by referring to our running instances.</p>"},{"location":"about/annotations/","title":"Annotations","text":"<p>UCE is compatible with a variety of annotations, provided they exist within the UIMA format. Within UCE, these annotations are used situationally for features or search enhancements, depending on the annotation.</p> <p>Below you will find an ever-expanding list of importable and compatible annotations within UCE, ranging from standard Named-Entity annotations to more situational taxon or time annotations. All of these annotations can be generated and annotated within the corpus through the Docker Unified UIMA Interface.</p> OCR <p>Since much of the literature has yet to be digitized, UCE provides support for corpora containing documents that have undergone Optical Character Recognition (OCR) extraction. These annotations assist in reconstructing the physical layout of the pages within UCE. More Details</p> Sentence <p>Divides the documents into their respective sentences. More Details</p> Named-Entity <p>Extracts named entities from a document, categorizing them into four types: organization (ORG), person (PER), location (LOC), and miscellaneous (MISC). More Details</p> Lemma, POS &amp; Morphological Features <p>Lemmatization reduces inflected words to their root form. Within UCE, searches are enhanced by considering these root forms. More Details</p> Semantic Role Labels (SRL) <p>SRL identifies semantic relations between the lexical constituents of a sentence, assigning labels to words or phrases that indicate their semantic roles, such as agent, goal, or result. More Details</p> Time <p>Extracts temporal expressions, including time and date formats, from a document, analogous to Named-Entity Recognition tasks. More Details</p> UceDynamicMetadata <p>Offers a dynamic and easy way to annotate key-value filters, which are then imported and used within UCE for the creation of custom filters. More Details</p> Taxon <p>The recognition of unambiguous names of biological entities is referred to as a taxon. Herein, UCE supports the import of multiple model-annotations, such as GNFinder or Gazetteer. More Details</p> WikiLinks <p>Maps potential words and phrases to their corresponding Wikidata URLs, facilitating the retrieval and access of additional information. More Details</p> UnifiedTopic <p>Extracts topics from a document in the form of a list of keywords or categories, which can be used to summarize the content or identify its main theme. The list of categories depends on the model used for annotation. More Details</p> GeoNames <p>The recognition of locations within texts and their annotation with hierarchical data, alternate and historical names, and tagging with unique identifiers. More Details</p> Logical Links <p>Link documents, annotations, and even texts to other entities so that they are connected with a defined edge and weight. UCE thus enables the grouping and linking of any entity with any other entity. More Details</p> Negation <p>Identifies and marks negation cues, their scopes, and affected events or concepts within text. This helps to determine when statements express the absence, denial, or opposite of something mentioned. More Details</p> Emotion <p>Detects emotional expressions or affective states (such as joy, anger, fear, or sadness) conveyed within text segments. These annotations can be used to study emotional tone and affective communication. More Details</p> Sentiment <p>Analyzes the overall sentiment polarity of text (positive, negative, or neutral), enabling corpus-wide mood analysis or evaluation of opinions expressed in documents. More Details</p>"},{"location":"about/ontologies/","title":"Ontologies","text":"<p>UCE already supports a collection of ontologies developed across various projects. These ontologies are fully supported out of the box, requiring no additional development within UCE to be functional.</p> <p>We are continuously working to expand this list\u2014similar to our efforts with compatible annotations. If you would like to contribute an ontology, you are welcome to submit a pull request or contact us directly.</p> <p>All ontologies must be in RDF or OWL format.</p> Biological Ontologies <p>As part of the BIOfid project, taxonomists have curated new biological taxonomies and extended existing ones. This ontology database builds upon the GBIF ontology and includes additional manually created taxonomies.  More Details</p>"},{"location":"about-us/about-us/","title":"About Us","text":"<p>The Unified Corpus Explorer is being developed by the Text Technology Lab at Goethe University, Frankfurt, Germany. It is part of the lab's effort to unify and manage the heterogeneous and large-scale data landscape of NLP, making it accessible to everyone. UCE started its development in January 2024.</p>"},{"location":"about-us/about-us/#team","title":"Team","text":"<ul> <li> <p>Prof. Dr. Alexander Mehler</p> <p>Alexander Mehler is Professor of Computational Humanities / Text Technology at Goethe University Frankfurt, where he leads the Text Technology Lab (TTLab).</p> </li> <li> <p>Kevin B\u00f6nisch</p> <p>Member of the Text Technology Lab (01/2024-10/2025) and Lead Developer of UCE. He created the project and began its development in 2024.</p> </li> <li> <p>Giuseppe Abrami</p> <p>Member of the Text Technology Lab and Lead Developer of the Docker Unified UIMA Interface, UCE's sibling software.</p> </li> <li> <p>Manuel Schaaf</p> <p>Member of the Text Technology Lab and Developer of UCE.</p> </li> <li> <p>Leon Hammerla</p> <p>Member of the Text Technology Lab and Developer of UCE, specifically in the context of one of its projects.</p> </li> <li> <p>Katrin Peikert</p> <p>Developer of UCE, specifically in the context of UCE's associated BIOfid project.</p> </li> <li> <p>Bhuvanesh Verma</p> <p>Member of the Text Technology Lab and Developer of UCE.</p> </li> </ul>"},{"location":"about-us/about-us/#contact","title":"Contact","text":"<p>Questions and issues related to development can be raised in the official GitHub repository. For other inquiries or to get in touch\u2014such as regarding projects or integrations\u2014please use the official lab's contact page.</p>"},{"location":"development/developer-code/","title":"Developer Code","text":"<p>Within UCE, we use several programming languages, including Java and Python, as well as multiple databases, each with its own query language, such as SQL and SPARQL. The core application, however, is written in Java, and thus the primary focus of development is on Java.</p> <p>Any developer who wishes to contribute to UCE should read the Code of (Development) Conduct. </p> <p>Also, feel free to \u2b50 the repo and join our Discord server to engage with the other developers and users of UCE.</p>"},{"location":"development/developer-code/#1-git-management","title":"1. Git Management","text":"<p>Fork the UCE repository. Development is carried out on the <code>develop</code> branch. For larger features or features that affect multiple parts of the codebase, consider creating a dedicated feature branch. However, in general, avoid creating separate branches for every minor feature. All changes must be submitted via a pull request.</p> <p>No-break Policy</p> <p>Your changes should never break the application for others in any way! Also check your (local) configurations!</p>"},{"location":"development/developer-code/#2-uce-is-generic-but-flexible","title":"2. UCE is Generic but Flexible","text":"<p>UCE is not a tool designed for a single specific use case or application. Therefore, when developing UCE, always consider how other domains, projects, and use cases might benefit from your contribution. </p> <p>Additionally, if your feature relies on situational conditions\u2014such as specific annotations or user configurations\u2014UCE must still function properly even when your feature is not active or cannot operate due to missing requirements. UCE must always adapt flexibly to the user's environment and configuration.</p>"},{"location":"development/developer-code/#3-keep-it-simple","title":"3. Keep it Simple","text":"<p>This isn't rocket science. There\u2019s no benefit in abstracting a triangle into a geometric shape, then into a shape, then into a unit, then into an object, and finally into an accumulation of atoms. Keep it simple\u2014abstract only when necessary. Code redundancy should be avoided, but it's not a plague. Don\u2019t overthink it too much. No one wants to traverse through 10 methods just to finally reach the actual logic.</p>"},{"location":"development/developer-code/#4-no-hardcoded-sql","title":"4. No Hardcoded SQL","text":"<p>Well, at least wherever applicable. We use Hibernate as an ORM within UCE, which should make writing hardcoded queries obsolete. For longer and more complex procedures, use PostgreSQL\u2019s stored procedures. </p> <p>However, in certain situations, you may encounter raw SQL strings in the source code\u2014for example, for dynamic interpolation or other necessities (I'm sure the comments will provide detailed reasoning ;-). As a rule of thumb: avoid hardcoding whenever possible (this guideline does not apply to SPARQL queries).</p> <p>For the creation of indexes, triggers, and similar database elements, you\u2019ll find a <code>database</code> folder within the repository. Place your SQL scripts there\u2014they will be executed automatically when UCE starts.</p>"},{"location":"development/developer-code/#5-to-var-or-not-to-var","title":"5. To <code>var</code> or not to <code>var</code>","text":"<p>For now, we\u2019ve agreed to leave this up to you. Personally, I find writing out longer types in Java annoying and hard to read (looking at you <code>ArrayList&lt;ArrayList&lt;Tuple&lt;String, Integer&gt;&gt;&gt;</code>), but rules like \"use <code>var</code> when the type is easy, otherwise write it out\" just make you overthink things that really aren\u2019t important.</p> <p>Do as you like (for now).</p>"},{"location":"development/developer-code/#6-ai","title":"6. AI","text":"<p>You can use whatever tools or sources you prefer for development\u2014just make sure that, if a source needs to be cited, you cite it appropriately. Also, double-check any AI-generated code and adapt it to match the style and structure of our existing codebase.</p> <p>Be sure to delete useless comments often generated by AI models, like:</p> <pre><code>.class {\n    background-color: gray; /* Set the background color to gray */\n}\n</code></pre> <p>Comments like these show poor form and clutter the codebase with unnecessary noise. But, talking about comments...</p>"},{"location":"development/developer-code/#7-comments","title":"7. Comments","text":"<p>Every Programmer</p> <p>\u201cMy methods don\u2019t need comments, they speak for themselves.\u201d </p> <p>No, they don't speak for themselves and yes, they need comments. Write small, precise, and concise comments that explain the flow of the code.</p> <p>However, don\u2019t overdo the Javadoc. If a private class is 2,000 lines long and 1,600 of those are comments, you\u2019re doing something wrong.</p>"},{"location":"development/developer-code/#8-spring-framework","title":"8. Spring Framework","text":"<p>UCE is built using the Spring framework and Maven. As such, we use Dependency Injection with services.</p> <p>These services are to be created in the <code>uce.common</code> submodule, then registered in <code>SpringConfig.java</code> for DI, and should encapsulate all logic related to their specific functionality. For example, you will never find a request to our PostgreSQL database outside of the designated database service (as always, with exceptions).</p> <p>For those who haven\u2019t worked with these patterns or frameworks before: familiarize yourself with the codebase and replicate the patterns from already existing services, logic classes, and other components.</p>"},{"location":"development/developer-code/#9-models-are-models","title":"9. Models are Models","text":"<p>Complementing point 8, we oppose the idea of coupling logic with models. E.g., each of the data classes associated with Hibernate\u2014and thus the database\u2014are purely data models, akin to structs. Through services, we fetch, modify, transform and save them.</p> <p>So remember:</p> <ul> <li>Logic belongs in services.  </li> <li>(Persistent) Data belongs in models.</li> <li>Everything in between is utility, configs, routes or special cases.  </li> </ul> <p>Keep them separated.</p>"},{"location":"development/developer-code/#10-have-fun","title":"10. Have Fun","text":"<p>This list sounds much stricter than it actually is, and it might give the impression that UCE was developed perfectly following these rules\u2014which it wasn\u2019t. We all work under time pressure, make mistakes, learn over time, write \"TODO: cleanup later\", and then never get around to cleaning it up.</p> <p>So take this as a guideline\u2014something to strive for\u2014but remember: then the actual coding happens.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>In general, UCE currently consists of several microservices, each dockerized and orchestrated via Docker Compose to form the application that is UCE. Among these services, some are obligatory (must-haves) and some are optional (specific use cases).</p> Service Description Obligatory Web Portal The web portal provides access to UCE for the user. It is the heart of UCE, communicating with and orchestrating all other services. \u2705 PostgreSQL DB The PostgreSQL database is the main database within UCE. It stores all data in a structured way and adds vector support through the pgvector extension. \u2705 Corpus Importer              The Importer handles the importing of UIMA-annotated corpora. Given a path, it will load UIMA files and project them into the UCE environment.             Without the importer, there is currently no other way to get data into UCE              (IO REST endpoints in UCE exist, but aren't production-ready yet).          \u2705 RAG Service The RAG service is a Python web server that primarily enables access to machine learning and AI models. It is required to calculate embeddings and enable the RAG bot. \u274c Sparql Service The (Fuseki) Sparql service allows the integration of ontological hierarchies in RDF or OWL format into UCE's searches. \u274c RAG Service The RAG service is a Python web server that primarily enables access to machine learning and AI models. It is required to calculate embeddings and enable the RAG bot. \u274c S3 MinIO Storage            The MinIO storage enables the storage of imported UIMA documents, allowing users to download any document along with its annotations in XMI format.        \u274c Keycloak            The Keycloak authentication server manages UCE\u2019s user and role systems. Once integrated, UCE provides user logins and access restrictions. The Keycloak service includes its own admin portal and a comprehensive management interface.        \u274c <p>In the following, you will learn how to set up these services and how to get started with your own UCE instance for your data.</p> <p>Structure</p> <p>The following sections are split into two parts:</p> <ul> <li>Setting up UCE as a user (Docker). </li> <li>Setting up UCE as an active developer (local). </li> </ul> <p>Depending on your use case, you will either set up the development environment or simply build UCE, import your data, and use it.</p>"},{"location":"getting-started/configuration/","title":"Configurations","text":"<p>UCE has three different configuration levels and with it, three different config files. These levels are:</p> <ul> <li>INSTANCE </li> <li>CORPUS </li> <li>DEVELOPER</li> </ul> <p>If you're a user only setting up UCE via Docker, then the <code>DEVELOPER</code> level is of no interest to you. In the following, we outline the different configurations and their usage within UCE.</p> UCE Configuration (INSTANCE) <p>UCE is customizable in a variety of ways, including color scheme, corpora identity, metadata, and more. To inject your UCE instance with your configuration, the <code>uceConfig.json</code> file exists. Through it, you can model the UCE instance within JSON and then pass that <code>uceConfig.json</code> file into the Web Portal through the command line.</p> <p>You can copy the example <code>uceConfig.json</code> below and create your own configuration from it.</p> uceConfig.json uceConfig.json<pre><code>{\n  \"meta\": {\n    \"name\": \"John Doe Lab\",\n    \"version\": \"1.0.0\",\n    \"description\": \"The John Doe Lab works in the field of finance analysis and, in this context, gathers large amounts of data for their sentiment or entailment tasks. This data is made available through the &lt;b&gt;Finance&lt;/b&gt; corpus. Herein, ...\"\n  },\n  \"corporate\": {\n    \"team\": {\n      \"description\": \"The team behind the Finance corpus is part of the &lt;a target='_blank' href='https://www.john-doe-lab.org/'&gt;John Doe Lab&lt;/a&gt; of the Doe-University.\",\n      \"members\": [\n        {\n          \"name\": \"Prof. John Doe\",\n          \"role\": \"Supervisor\",\n          \"description\": \"Mr. Doe is the supervisor of the lab.\",\n          \"contact\": {\n            \"name\": \"Prof. Dr. John Doe\",\n            \"email\": \"doe@doe-university.de\",\n            \"website\": \"https://john-doe.org/team/john-doe/\",\n            \"address\": \"Doe-Street 10&lt;br/&gt;11111 Doe\"\n          },\n          \"image\": \"FILE::https://upload.wikimedia.org/wikipedia/commons/9/99/Sample_User_Icon.png\"\n        },\n      ]\n    },\n    \"contact\": {\n      \"name\": \"John Doe Lab\",\n      \"email\": \"doe@doe-university.de\",\n      \"website\": \"https://www.john-doe-lab.org/contact\",\n      \"address\": \"Doe-Street 10&lt;br/&gt;11111 Doe\"\n    },\n    \"website\": \"https://www.john-doe-lab.org\",\n    \"logo\": \"FILE::https://upload.wikimedia.org/wikipedia/commons/9/99/Sample_User_Icon.png\",\n    \"name\": \"John Doe Lab\",\n    \"primaryColor\": \"#00618f\",\n    \"secondaryColor\": \"rgba(35, 35, 35, 1)\",\n    \"imprint\": \"&lt;p&gt;No imprint set.&lt;/p&gt;\"\n  },\n  \"settings\": {\n    \"rag\": {\n      \"models\": [\n        {\n          \"model\": \"ollama/gemma3:latest\",\n          \"url\": \"http://your.ollama.server.com/\",\n          \"apiKey\": \"\",\n          \"displayName\": \"Gemma3 (4.3B - Google)\"\n        },\n        {\n          \"model\": \"openai/o4-mini\",\n          \"url\": \"\",\n          \"apiKey\": \"YOUR_OPENAI_API_KEY\",\n          \"displayName\": \"GPT-4o-mini (OpenAI)\"\n        }\n      ]\n    },\n  \"analysis\": {\n    \"enableAnalysisEngine\": false\n  },\n  \"authentication\": {\n    \"isActivated\": false,\n    \"publicUrl\": \"http://localhost:8080\",\n    \"redirectUrl\": \"http://localhost:4567/auth\"\n  },\n}\n</code></pre> MetaCorporateSettingsAnalysisAuthentication <p> Property Description name Name of your project or your lab, shown on the front page of the web portal. version Your personal version counts. description A description shown on the front page of the portal. Use it to describe your UCE instance. </p> <p> Property Description team Outline and display your team in a dedicated Teams-Tab within your UCE instance. team.description Describe the team working on this project. team.members[] Create a list of <code>member-objects</code> to model your team and each member. contact The contact information is shown in the footer of the webportal. Deposit contact information such as name, website and email for others to contact you through the UCE instance. website The website of your lab or corporation. logo The logo is shown in the top left of the web portal. You can inject the logo via a file path <code>FILE::{PATH}</code> (works with online paths as well) or directly through Base64-encoded images <code>BASE64::data:image/png;base64,{BASE64}</code>. name The name of your lab or corporation. primaryColor Set the primary color for the UCE web portal and model your color scheme. secondaryColor Set the secondary color for the UCE web portal and model your color scheme. imprint Fill in a full HTML page of your imprint which will then be available via button in the footer of your UCE instance. </p> <p> Property Description rag Set the settings for the RAGbot (if applicable). rag.models[] A list of supported LLMs that power the RAGBot for the user. All listed models will be available for the user via dropbox. rag.models.model A language model that UCE is supposed to power the RAGBot with. Currently, we support Ollama and OpenAI out of the box, so this name needs to be the actual model's id (e.g. ollama/gemma3:latest* or openai/gpt-4o-mini).        rag.models.url Needed if a local Ollama server is used. This is the base url to that server which will be used by the RAG Service to communicate with it. rag.models.apiKey Needed if the OpenAI API is used. Fill in your own OpenAI api key that will be used by the RAG Service for communication. rag.models.displayName The name the user sees for this model in the UCE webportal. </p> <p> Property Description enableAnalysisEngine Enable or disable the built-in analysis engine into UCE, which is powered through DUUI. </p> <p> Property Description isActivated Enable or disable the authentication server for UCE, allowing login and user access, which is powered through Keycloak. publicUrl This is the base url under which the Keycloak authentication server is reachable by UCE. If the default url or port was changed of the Keycloak Service, this needs to be adjusted. redirectUrl This is the base url of the running UCE webportal, which is then passed into Keycloak. The latter needs this url for communicating with its client's callbacks, in this case UCE. </p> <p>Within the source code, you also find a <code>defaultUceConfig.json</code> that you can mirror. This is also the configuration UCE uses if no explicit config is provided. Inject the <code>uceConfig.json</code> into the UCE web portal by means of command line arguments, as outlined in earlier sections.</p> Corpus Configuration (CORPUS) <p>As the name suggests, the <code>corpusConfig.json</code> holds metadata about a single corpus within UCE. Unlike the <code>uceConfig.json</code>, the corpus config is obligatory and needs to be imported by the Corpus-Importer.</p> <p>You can copy the example <code>corpusConfig.json</code> below and create your own configuration from it.</p> corpusConfig.json corpusConfig.json<pre><code>{\n  \"name\": \"Corpus_Name\",\n  \"author\": \"University Doe\",\n  \"language\": \"de-DE/en-EN/...\",\n  \"description\": \"The corpus was gathered as part of the John Doe project.\",\n  \"addToExistingCorpus\": true,\n\n  \"annotations\": {\n    \"annotatorMetadata\": false,\n    \"uceMetadata\": false,\n    \"logicalLinks\": false,\n\n    \"OCRPage\": false,\n    \"OCRParagraph\": false,\n    \"OCRBlock\": false,\n    \"OCRLine\": false,\n\n    \"srLink\": false,\n    \"lemma\": false,\n    \"namedEntity\": false,\n    \"sentence\": false,\n    \"sentiment\": false,\n    \"emotion\": false,\n    \"time\": false,\n    \"geoNames\": false,\n    \"taxon\": {\n      \"annotated\": false,\n      \"//comment\": \"[Are the taxons annotated with biofid onthologies through the 'identifier' property?]\",\n      \"biofidOnthologyAnnotated\": false\n    },\n    \"wikipediaLink\": false,\n    \"completeNegation\": false,\n    \"cue\": false,\n    \"event\": false,\n    \"focus\": false,\n    \"scope\": false,\n    \"xscope\": false,\n    \"unifiedTopic\": false\n  },\n  \"other\": {\n    \"//comment\": \"[Is this corpus also available on https://sammlungen.ub.uni-frankfurt.de/? Either true or false]\",\n    \"availableOnFrankfurtUniversityCollection\": false,\n\n    \"includeKeywordDistribution\": false,\n    \"enableEmbeddings\": false,\n    \"enableRAGBot\": false\n  }\n}\n</code></pre> Property Description name The name assigned to the corpus. author The entity or institution that created the corpus. language Languages included in the corpus, specified in locale format (e.g., \"de-DE\", \"en-EN\"). description A brief overview of the corpus and its purpose. addToExistingCorpus Boolean flag indicating whether to append this data to an existing corpus *(looked up by name)*, or whether a new corpus should be created. annotations Object outlining how the corpus was annotated and which annotation layers are available. annotations.annotatorMetadata Boolean flag indicating if metadata about the annotator (e.g., name, date, or tool used) is included. annotations.uceMetadata Boolean flag indicating if metadata per document is included (e.g. publishers, author etc.), which is done through its own UIMA-Typesystem. annotations.logicalLinks Boolean flag indicating if logical or structural links between annotation layers (e.g., reference chains or document relations) are included. This is also done through its own UIMA-Typesystem. annotations.OCRPage Boolean flag indicating if OCR data at the page level is included. annotations.OCRParagraph Boolean flag indicating if OCR data at the paragraph level is included. annotations.OCRBlock Boolean flag indicating if OCR data at the block level is included. annotations.OCRLine Boolean flag indicating if OCR data at the line level is included. annotations.srLink Boolean flag indicating if semantic role links (verb-argument structures) are annotated. annotations.lemma Boolean flag indicating if lemmatization (base forms of words) is performed. annotations.namedEntity Boolean flag indicating if named entities (e.g., persons, locations, organizations) are annotated. annotations.sentence Boolean flag indicating if sentence boundaries are annotated. annotations.sentiment Boolean flag indicating if sentiment analysis annotations (positive, neutral, negative) are included. annotations.emotion Boolean flag indicating if emotion annotations (e.g., anger, joy, sadness) are included. annotations.time Boolean flag indicating if temporal expressions (e.g., dates, time spans) are annotated. annotations.geoNames Boolean flag indicating if geographic names are annotated and linked to GeoNames identifiers. annotations.taxon Object containing details about taxon annotations. annotations.taxon.annotated Boolean flag indicating if taxons are annotated in the corpus. annotations.taxon.biofidOnthologyAnnotated Boolean flag indicating if taxons are annotated with BioFID ontologies through the <code>identifier</code> property. annotations.wikipediaLink Boolean flag indicating if Wikipedia links are included for entities or terms. annotations.completeNegation Boolean flag indicating if complete negation structures (negation cues and scopes) are annotated. annotations.cue Boolean flag indicating if linguistic cues (e.g., trigger words for negation or modality) are annotated. annotations.event Boolean flag indicating if event annotations (occurrences, actions, or states) are included. annotations.focus Boolean flag indicating if focus annotations (focus elements or highlighted text segments) are included. annotations.scope Boolean flag indicating if negation or modality scopes are annotated. annotations.xscope Boolean flag indicating if extended scopes (cross-sentence or multi-event) are annotated. annotations.unifiedTopic Boolean flag indicating if unified topic annotations (global thematic categories) are included. other Object containing additional corpus-related properties. The following flags require the setup of the RAG-Service. other.availableOnFrankfurtUniversityCollection Boolean flag indicating if the corpus is also available via the Frankfurt University Collection. other.includeKeywordDistribution Boolean flag indicating if keyword distribution data should be generated and cached upon import. other.enableEmbeddings Boolean flag indicating if embeddings (vector representations of texts) should be created and cached upon import. other.enableRAGBot Boolean flag indicating if the RAGBot feature (retrieval-augmented chatbot for corpus interaction) should be enabled. Common Configuration (DEVELOPER) <p>In the source code's <code>uce.common</code> module, you'll find a <code>common.conf</code> file. In it, you can adjust and edit any configurations needed to run the application, such as DB connection strings, API endpoints, and the like. To properly run UCE in a development setting, you need to ensure that all the local connection strings match your setup. For that, the most relevant ones are:</p> Property Description rag.webserver.base.url The base url to the RAG-service's webserver (if setup), e.g.: <code>http://localhost:5678/.</code> sparql.host The base url to the Sparql-service's webserver (if setup), e.g.: <code>http://localhost:3030/</code> sparql.endpoint The endpoint of the Sparql-service's webserver, e.g.: <code>my-ontology/sparql</code> postgresql.hibernate.connection.url The connection string to the Postgresql-DB-service, e.g.: <code>jdbc:postgresql://localhost:5433/uce</code> <p>You'll also find one more file, called <code>common-release.conf</code>. Since, for the release, most of the connections differ from the local setup (specifically in the docker compose network), the <code>common-release.conf</code> is used when building UCE with docker and has the same properties as its debug counterpart.</p>"},{"location":"getting-started/corpus-importer/","title":"Corpus Importer","text":"<p>Prerequisites</p> <p>This section requires that you have already set up the PostgreSQL DB and, preferably, the Web Portal. If not, refer to the respective documentation.</p> <p>The Corpus-Importer is a Java application that transforms and imports UIMA-annotated data from a local path into the UCE environment. Depending on the configuration, it also performs post-processing of the data, such as the creation of embedding spaces.</p> <p>UIMA</p> <p>If the data is not yet available in UIMA format, refer to the respective documentation, which also utilizes the Docker Unified UIMA Interface to transform, process, and annotate the data in UIMA format the best way possible. After having transformed your data, proceed here.</p>"},{"location":"getting-started/corpus-importer/#folder-structure","title":"Folder Structure","text":"<p>Having set up the database and the web portal (locally or via docker), all that is left to do is to tell the importer where to import from and start it.</p> <p>For this, the importer always requires the following folder structure:</p> Required Folder Structure<pre><code>\ud83d\udcc1 corpus_a\n\u2502   \ud83d\udcc4 corpusConfig.json\n\u2514\u2500\u2500\u2500\ud83d\udcc1 input\n    \u2502   \ud83d\udcc4 uima_doc_1.xmi\n    \u2502   \ud83d\udcc4 uima_doc_2.xmi\n    \u2502   \ud83d\udcc4 ...\n    \u2502   \ud83d\udcc4 uima_doc_n.xmi\n</code></pre> <p>where <code>corpusConfig.json</code> holds metadata, and the <code>input</code> folder contains the actual UIMA files for a single corpus.</p> <p>Input Structure</p> <p>As of now, the importer will recursively walk through the <code>input</code> folder, so every <code>.xmi</code> file in any subfolder will be considered.</p>"},{"location":"getting-started/corpus-importer/#user-setup","title":"User Setup","text":"<p>Open the <code>docker-compose.yaml</code> file (if you haven't created the <code>.env</code> file yet, see here) and locate the <code>uce-importer</code> service. Within it, mount all local paths to the corpora you want to import using the structure described above, and map them like so: </p> <pre><code>volumes:\n    - \"./path/to/my_corpora/corpus_a:/app/input/corpora/corpus_a\"\n    - \"./path/to/other_corpora/corpus_b:/app/input/corpora/corpus_b\"\n    - \"...\"\n</code></pre> <p>You can mount as many corpora as you like using the same structure. Remember that you can adjust the amount of threads used through the <code>.env</code> file.</p> <p>Afterwards, simply start the importer through the compose:</p> <pre><code>docker-compose up --build uce-importer\n</code></pre> <p>The importer will automatically import all corpora that is mounted to its local <code>/app/input/corpora/</code> volume.</p>"},{"location":"getting-started/corpus-importer/#developer-setup","title":"Developer Setup","text":"<p>In the source code, identify the module <code>uce.corpus-importer</code> and set up your IDE:</p> <p>Setup</p> <ul> <li>Add a new <code>Application</code> configuration  </li> <li>UCE is developed in Java 21 </li> <li>Set <code>-cp corpus-importer</code> </li> <li>Main class: <code>org.texttechnologylab.App</code> </li> <li>CLI arguments are obligatory:<ul> <li><code>-src \"./path/to/your_corpus/\"</code></li> <li><code>-num 1</code></li> <li><code>-t 1</code></li> </ul> </li> <li>Maven should automatically download and index the dependencies. If, for some reason, it does not, you can force an update via <code>mvn clean install -U</code> (in IntelliJ, open <code>Execute Maven Goal</code>, then enter the command).</li> </ul> <p>Open the <code>common.conf</code> file and adjust the database connection parameters to match your database (port, host, etc.). Now start the importer and import your corpus. Refer to CLI Arguments for a full list of possible parameters.</p> <p>Logs</p> <p>The importer logs to both the PostgreSQL database (tables <code>uceimport</code> and <code>importlog</code>) and the local <code>logs</code> directory within the container. Both logs also appear in the standard output of the console.</p>"},{"location":"getting-started/corpus-importer/#cli-arguments","title":"CLI Arguments","text":"Argument Description <code>-src</code> <code>--importSrc</code> The path to the corpus source where the UIMA-annotated files are stored. <code>-srcDir</code> <code>--importDir</code> Unlike <code>-src</code>, <code>-srcDir</code> is the path to a directory that holds multiple importable <code>src</code> paths. The importer will check for folders within this directory, where each folder should be an importable corpus with a corpusConfig.json and its input UIMA-files. Those are then imported. <code>-num</code> <code>--importerNumber</code> When starting multiple importers, assign an id to each instance by counting up from 1 to n (not relevant as off now, just set it to 1). <code>-t</code> <code>--numThreads</code> UCE imports asynchronous. Decide with how many threads, e.g. 4-8-16. By default, this is single threaded. <code>-view</code> <code>--casView</code> Name of the CAS view to import from. If not set, the default view (initial view) is used. Adjust this only if you're familiar with CAS views and UIMA. Otherwise, you probably don't need this."},{"location":"getting-started/fuseki-sparql/","title":"Sparql Service","text":"<p>The (Fuseki) Sparql service allows the integration of ontological hierarchies in RDF or OWL format into UCE's searches. It does so through a SPARQL graph database which stores RDF triplets.</p> <p>Out-of-the-box Ontologies</p> <p>Please refer to our list of plug-and-play Ontologies to see what ontologies have already been made accessible in UCE without you having to  develop anything. We are continuously expanding plug-and-play compatibility with other ontologies, and if you want to incorporate your own compatibility into UCE, feel free to make a pull request or get in touch!</p>"},{"location":"getting-started/fuseki-sparql/#user-setup","title":"User Setup","text":"<p>For this, the following parameters in the <code>.env</code> file need to be set correctly:</p> <pre><code>TDB2_DATA=./../tdb2-database\nTDB2_ENDPOINT=tdb2-database-name\n</code></pre> <p>where <code>TDB2_DATA</code> is the local path to your TDB2 database and <code>TDB2_ENDPOINT</code> is the name under which this database will be queryable. This endpoint must match the first part of the <code>sparql.endpoint</code> parameter in the <code>common.conf</code> file (<code>tdb2-database-name/sparql</code>).</p> <p>Afterwards, simply start the <code>uce-fuseki-sparql</code> container:</p> <pre><code>docker-compose up --build uce-fuseki-sparql\n</code></pre>"},{"location":"getting-started/fuseki-sparql/#developer-setup","title":"Developer Setup","text":"<p>You can easily set up the SPARQL database as outlined in the User Setup. However, for more efficient testing and to take advantage of the web interface provided by Apache Jena, it may be advisable to install a local Fuseki SPARQL instance as well. For that:</p> <ul> <li>Download the latest Apache Jena Binary Distribution.</li> <li>Unzip the archive, navigate to the folder, and start the SPARQL server (requires Java to be installed): <pre><code>java -Xmx8G -jar fuseki-server.jar --update\n</code></pre> <code>--update</code> makes the database persistent.</li> <li>By default, the SPARQL database and its web interface are accessible at <code>http://localhost:3030</code>.</li> <li>You can now create a new database through the Web UI. The name of the database will also serve as the endpoint <code>/db_name</code>.</li> </ul> <p>Web UI &amp; Import Option</p> <p>When installing locally and opening the web UI, you can create a new database and use the <code>Import</code> button to import RDF and OWL files directly. Once the database has been populated, you will find the corresponding <code>TDB2</code> database as a folder within your program directory (likely under <code>run/databases</code>). You can simply mount this folder as the <code>TDB2_DATA</code> location.</p> tdb.lock <p>The database will generate a <code>tdb.lock</code> file to prevent multiple instances from accessing the same TDB2 database simultaneously. Be mindful of this, as it often leads to errors when overlooked.</p> Disk Space <p>When importing large volumes of new triplets, the SPARQL database generates log and transaction files, which can quickly consume significant disk space and bloat the database. You can heavily reduce the size and reclaim unnecessary space by using the <code>/compact</code> endpoint once your importing is finished.</p>"},{"location":"getting-started/keycloak/","title":"Keycloak","text":"<p>The Keycloak service integration adds an authentication layer to UCE, providing login, user, and role management systems. This integration is still a work in progress\u2014while not all features are fully implemented, the setup and core functionality already work reliably.</p> <p>For this service, the setup process is the same for both users and developers, as it relies on Docker images.</p>"},{"location":"getting-started/keycloak/#setup","title":"Setup","text":"<p>The following parameters in the <code>.env</code> file must be set correctly:</p> <pre><code>KC_REALM_IMPORT_PATH=./auth/uce-realm.json\nKC_ADMIN_USERNAME=admin\nKC_ADMIN_PW=admin\n</code></pre> <p>where <code>KC_REALM_IMPORT_PATH</code> is the local path to your realm configuration (essentially the configuration for your personal Keycloak instance which you can find the UCE source), and <code>KC_ADMIN_USERNAME</code> and <code>KC_ADMIN_PW</code> define the default admin credentials required for login.</p> <p>If you modify any parameters in the default <code>uce-realm.json</code>, you must also update the <code>common.conf</code> and <code>common-release.conf</code> files accordingly. However, it is not recommended to do so. Instead, use the default configuration and make adjustments through the web UI after startup.</p> <p>Afterwards, simply start the <code>uce-keycloak-auth</code> container:</p> <pre><code>docker-compose up --build uce-keycloak-auth\n</code></pre> <p>Keycloak will start at <code>http://localhost:8080</code>, where you can access the admin web UI. To log in, use the <code>KC_ADMIN_USERNAME</code> and <code>KC_ADMIN_PW</code> credentials defined in the <code>.env</code> file. In the admin cockpit, you will find a realm named uce, which is used by UCE.</p> <p>Changes</p> <p>If you modify any of the default configurations-including the realm name, the client name (which is uce-web), or the client secret-you must update these parameters accordingly in the <code>common.conf</code> files within the UCE source.</p> <p>To enable authentication in UCE, set the <code>authentication.isActivated</code> flag to <code>true</code> in the UCE configuration. Once enabled, a login button will appear in UCE, and certain features-such as the RAGBot-will only be accessible to authenticated users.</p> <p>Exposing the Port</p> <p>The Keycloak service must be exposed outside the Docker Compose network, as UCE relies on public URL callbacks. Additionally, the port needs to be exposed to allow access to the admin UI.</p>"},{"location":"getting-started/rag-service/","title":"RAG Service","text":"<p>The RAG service is a Python web server running on Flask, which acts as the gateway to modern AI, NLP, and ML technologies. It handles the creation of embeddings, querying large language models (LLMs) for the RAGbot, and similar operations.</p> <p>High Resources</p> <p>Depending on the use case and the available machine, the RAG service may be resource-intensive, particularly in terms of RAM and GPU usage.</p>"},{"location":"getting-started/rag-service/#user-setup","title":"User Setup","text":"<p>For this, it is assumed that the repository has already been cloned in a prior step. Afterwards, simply start the service via Docker Compose:</p> <pre><code>docker-compose up --build uce-rag-service\n</code></pre> <p>The RAG service should now be up and running on the port mapped in the Dockerfile. See further down for information about model usage and customizable settings of the service.</p>"},{"location":"getting-started/rag-service/#developer-setup","title":"Developer Setup","text":"<p>After cloning the repository, navigate to the <code>rag</code> folder. There, create a new Python environment using a tool of your choice:</p> <pre><code>python -m venv env\n</code></pre> <p>The name <code>env</code> is already included in <code>.gitignore</code>, so it's recommended to use that name if possible.</p> <p>Activate the environment:</p> <pre><code>source env/bin/activate\n</code></pre> Activate on Windows <p>On Windows, activate the environment with the following command: </p><pre><code>.\\env\\Scripts\\activate\n</code></pre><p></p> <p>Then, install the dependencies and start the service:</p> <pre><code>pip install -r requirements.txt\npython src/webserver.py\n</code></pre>"},{"location":"getting-started/rag-service/#settings","title":"Settings","text":"<p>The RAGBot can be configured with different LLMs for inference and user interaction. It is recommended to use either an OpenAI model (in which case an OpenAI API key must be set) or a locally hosted Ollama server.</p> <p>In both cases, the configuration must be defined in the UCE Configuration. The relevant excerpt is shown below:</p> Example RAGBot settings within uceConfig.json uceConfig.json<pre><code>{\n  \"settings\": {\n    \"rag\": {\n      \"models\": [\n        {\n          \"model\": \"ollama/gemma3:latest\",\n          \"url\": \"http://ollama.llm.texttechnologylab.org/\",\n          \"apiKey\": \"\",\n          \"displayName\": \"Gemma3 (4.3B - Google)\"\n        },\n        {\n          \"model\": \"ollama/deepseek-r1:latest\",\n          \"url\": \"http://ollama.llm.texttechnologylab.org/\",\n          \"apiKey\": \"\",\n          \"displayName\": \"DeepSeek-R1 (7.6B - DeepSeek)\"\n        },\n        {\n          \"model\": \"openai/o4-mini-2025-04-16\",\n          \"url\": \"\",\n          \"apiKey\": \"YOUR_API_KEY\",\n          \"displayName\": \"OpenAI's O4 Mini\"\n        }\n      ]\n    }\n  }\n}\n</code></pre> <p>For OpenAI models, you can specify any model name listed in the OpenAI model catalog. UCE provides the option for users to select which model they want to use from the list of models defined in the UCE configuration.</p>"},{"location":"getting-started/s3-storage/","title":"S3 MinIO Storage","text":"<p>The MinIO storage enables the storage of UIMA XMI files during import, allowing users to download them through UCE. Each file includes the original text along with all annotations in a standardized format.</p> <p>Additional features leveraging the S3 storage are planned, but currently, it is used solely for downloading files within UCE\u2019s document reader.</p> <p>For this service, the setup process is the same for both users and developers, as it relies on Docker images.</p>"},{"location":"getting-started/s3-storage/#setup","title":"Setup","text":"<p>No parameters need to be set in the <code>.env</code> file. However, if you want to start the service with an existing S3 database, adjust the following line:</p> <pre><code>MINIO_STORAGE_DATA=./../minio/data_exports/2025-06-12/minio-backup\n</code></pre> <p>to match the location of your existing data. This will mount the preexisting storage into the service.</p> <p>Then, simply start the container:</p> <pre><code>docker-compose up --build uce-minio-storage\n</code></pre> <p>Upon start, the S3 storage comes with a Web UI under <code>http://localhost:9000</code> or <code>http://localhost:9001</code>. The default credendtials can be found in the <code>docker-compose.yaml</code> </p> <pre><code>MINIO_ROOT_USER=admin\nMINIO_ROOT_PASSWORD=12345678\n</code></pre> <p>Exposing</p> <p>If you plan to expose this server outside the Docker Compose network (which is neither recommended nor required for UCE), make sure to change the default admin credentials!</p>"},{"location":"getting-started/uima-duui/","title":"UIMA & DUUI","text":"<p>As mentioned, UCE works on UIMA-annotated data, which enables the standardized storing, processing, and further utilization of your corpora and annotations. The easiest way to annotate large amounts of data in parallel is through the Docker Unified UIMA Interface (DUUI).</p> <p>In the following, we will first briefly outline the technologies, explain their advantages, and finally present small tutorials to help you get started. As a result, you will build the following pipeline, which leads to your UCE instance:</p> <pre><code>graph LR\n    A[Corpus] ---&gt; F{Format Supported?}\n    F ---&gt;|Yes| D[\u2699 DUUI]\n    F ---&gt;|No| C[Converter]\n    C ---&gt; D[\u2699 DUUI]\n    T[\"Typesystem (optional)\"] ---&gt; D[\u2699 DUUI]\n    I[Importer] ---&gt; E[\ud83c\udf0d UCE]\n    D ---&gt; I</code></pre> <p>It is not required to use DUUI to produce UIMA-annotated data. You can choose any technique you prefer\u2014at the end of the day, UCE simply needs UIMA files. DUUI can read a wide variety of distinct corpus formats. You can check the list of available formats here. If your format is not supported, you will need to convert it yourself; the tutorial below explains how to create UIMA documents and add text and annotations to them. We have our own typesystem, which details a broad range of different annotations and should cover nearly all use cases. You can find the typesystem here.</p>"},{"location":"getting-started/uima-duui/#uima","title":"UIMA","text":"<p>UIMA (Unstructured Information Management Architecture), is a framework designed to process and analyze large volumes of unstructured data, such as text, audio, images, and video. It provides a standardized and flexible platform to integrate various analysis components, making it easier to build complex systems for tasks like natural language processing (NLP), information extraction, and machine learning.</p>"},{"location":"getting-started/uima-duui/#duui","title":"DUUI","text":"<p>DUUI (Docker Unified UIMA Interface) is a platform designed to efficiently process large media corpora (mainly text, but offers also support for other media types such as video, images, and audio). It builds on the UIMA framework, using it to manage annotations, and leverages container technologies to integrate a variety of NLP tools. DUUI supports both horizontal scaling (distributing processing across multiple machines) and vertical scaling (optimizing resource use on a single machine), making it highly scalable for handling big data. It accommodates diverse NLP tools and programming languages, abstracting their differences through a unified interface, and ensures reproducibility by tracking processing pipelines. Additionally, DUUI provides robust monitoring and error-reporting features to manage large-scale tasks and is designed for ease of use, making advanced NLP accessible to users with varying technical expertise, including non-experts in research fields like digital humanities, biodiversity etc. For more information go to the github repository of the original DUUI project. For a list of all available components check the DUUI-Components repository.</p>"},{"location":"getting-started/uima-duui/#tut-1","title":"Tut 1","text":"<p>This example demonstrates a simple annotation process: (1) creating a UIMA document, (2) setting its text, (3) splitting the text at the whitespace level, and then (4) adding token annotations to the UIMA document.</p> <pre><code>import org.apache.uima.UIMAException;\nimport org.apache.uima.jcas.JCas;\nimport org.apache.uima.jcas.JCasFactory;\nimport org.apache.uima.jcas.tcas.Annotation;\nimport org.apache.uima.cas.FSIterator;\n\npublic class JCasExample {\n    public static void main(String[] args) {\n        try {\n            // Step 1: Create the JCas object\n            JCas jc = JCasFactory.createJCas();\n\n            // Step 2: Set the sofa string (document text)\n            String text = \"This is a sample text: I like DUUI and UCE\";\n            jc.setDocumentText(text);\n\n            // Step 3: Add token annotations by splitting on whitespace\n            String[] tokens = text.split(\"\\\\s+\"); // Split by one or more whitespace characters\n            int currentPosition = 0;\n\n            for (String token : tokens) {\n                // Find the starting position of the current token in the original text\n                int begin = text.indexOf(token, currentPosition);\n                int end = begin + token.length();\n\n                // Step 4: Create an annotation for the token and add the annotation to the JCas\n                Annotation annotation = new Annotation(jc, begin, end);\n                annotation.addToIndexes();\n\n                // Update the current position to search for the next token\n                currentPosition = end;\n            }\n\n            // Step 5 (optional): Print the annotations\n            FSIterator&lt;Annotation&gt; iterator = jc.getAnnotationIndex(Annotation.type).iterator();\n            while (iterator.hasNext()) {\n                Annotation ann = iterator.next();\n                System.out.println(\"Token: \" + ann.getCoveredText() + \" [\" + ann.getBegin() + \", \" + ann.getEnd() + \"]\");\n            }\n        } catch (UIMAException e) {\n            e.printStackTrace();\n        }\n    }\n}\n</code></pre>"},{"location":"getting-started/uima-duui/#tut-2","title":"Tut 2","text":"<p>This example demonstrates the initialization and execution of a DUUI pipeline for processing a corpus. It includes a simple language detection component to annotate the language of the documents. The example covers the following steps: (1) Define the input and output corpus paths, (2) Declare a pipeline reader, (3) Initialize a Lua context, (4) Initialize the composer, (5) Initialize drivers and add them to the pipeline, (6) Initialize components and add them to the pipeline, (7) Add a writer to the pipeline, (8) Run the pipeline.</p> <pre><code>import org.dkpro.core.api.resources.CompressionMethod;\nimport org.dkpro.core.io.xmi.XmiWriter;\nimport org.junit.jupiter.api.Test;\nimport org.texttechnologylab.DockerUnifiedUIMAInterface.DUUIComposer;\nimport org.texttechnologylab.DockerUnifiedUIMAInterface.driver.*;\nimport org.texttechnologylab.DockerUnifiedUIMAInterface.io.DUUIAsynchronousProcessor;\nimport org.texttechnologylab.DockerUnifiedUIMAInterface.io.DUUICollectionReader;\nimport org.texttechnologylab.DockerUnifiedUIMAInterface.io.reader.DUUIFileReaderLazy;\nimport org.texttechnologylab.DockerUnifiedUIMAInterface.lua.DUUILuaContext;\n\nimport java.io.File;\n\nimport static org.apache.uima.fit.factory.AnalysisEngineFactory.createEngineDescription;\n\npublic class PipelineTest {\n\n    @Test\n    public void EUBooks() throws Exception {\n\n        int iWorker = 1;\n        // Step 1: Define input path to a corpus and output path, where processed corpus should be saved\n        String sInputPath = \"/tmp/EUBook/input\";\n        String sOutputPath = \"/tmp/EUBook/output\";\n\n        String sSuffix = \"xmi.bz2\";\n\n        // Step 2: Define Reader\n        DUUICollectionReader pReader = new DUUIFileReaderLazy(sInputPath, sSuffix, sOutputPath, \".xmi.bz2\", 1);\n\n        // Asynchronous Reader for the Input Files\n        DUUIAsynchronousProcessor pProcessor = new DUUIAsynchronousProcessor(pReader);\n        new File(sOutputPath).mkdir();\n\n        // Step 3: Initialize Lua Context\n        DUUILuaContext ctx = new DUUILuaContext().withJsonLibrary();\n\n        // Step 4: Initialize Composer\n        DUUIComposer composer = new DUUIComposer()\n                .withSkipVerification(true)     // skik component verification\n                .withLuaContext(ctx)            // set lua context\n                .withWorkers(iWorker);         // set threads for the composer\n\n        // Step 5: Initialize all needed Drivers (based on what kind of components one would like to use)\n        DUUIDockerDriver docker_driver = new DUUIDockerDriver();\n\n        // add drivers to the composer\n        composer.addDriver(docker_driver); \n\n        // Step 6: Initialize components and add them to the composer scope\n        DUUIPipelineComponent componentLang = new DUUIDockerDriver\n                //DUUIPipelineComponent componentLang = new DUUIDockerDriver\n                .Component(\"docker.texttechnologylab.org/languagedetection:0.5\")\n                .withImageFetching()\n                .withScale(iWorker)\n                .build();\n        composer.add(componentLang);\n\n        // Step 7: Add a writer to the pipeline\n        composer.add(new DUUIUIMADriver.Component(createEngineDescription(XmiWriter.class,\n                XmiWriter.PARAM_TARGET_LOCATION, sOutputPath,\n                XmiWriter.PARAM_PRETTY_PRINT, true,\n                XmiWriter.PARAM_OVERWRITE, true,\n                XmiWriter.PARAM_VERSION, \"1.1\",\n                XmiWriter.PARAM_COMPRESSION, CompressionMethod.BZIP2\n        )).withScale(iWorker).build());\n\n        // Step 8: Run the pipeline\n        composer.run(pProcessor, \"eubook\");\n    }\n}\n</code></pre>"},{"location":"getting-started/webportal/","title":"Web Portal","text":"<p>The heart of UCE is its webportal, which, alongside the Postgresql database, are the primary microservices we will setup first, as these services are obligatory.</p>"},{"location":"getting-started/webportal/#user-setup","title":"User Setup","text":"<p>It's always best to build from source, so clone the UCE repository:</p> <pre><code>git clone https://github.com/texttechnologylab/UCE.git\n</code></pre> <p>In the root folder, create a <code>.env</code> file that holds the variables for the <code>docker-compose.yaml</code> file. E.g.:</p> .env<pre><code>UCE_CONFIG_PATH=./../uceConfig.json\nJAVA_OPTIONS=-Xmx8g -Xms8g -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:+ParallelRefProcEnabled -XX:+UnlockExperimentalVMOptions -XX:+UseStringDeduplication\nTDB2_DATA=./../tdb2-database\nTDB2_ENDPOINT=tdb2-database-name\nIMPORTER_THREADS=1\nPOSTGRESQL_CONFIG=./database/postgresql.conf\nKC_REALM_IMPORT_PATH=./auth/uce-realm.json\nKC_ADMIN_USERNAME=admin\nKC_ADMIN_PW=admin\n</code></pre> <ul> <li><sup><code>UCE_CONFIG_PATH</code>: The local path to the UceConfig file, which injects custom settings into the UCE instance. If none is provided, UCE defaults to a standard configuration.</sup> </li> <li><sup><code>JAVA_OPTIONS</code>: Relevant only if the SPARQL microservice is enabled. Specifies the maximum size, in bytes, of the memory allocation pool for the service.</sup> </li> <li><sup><code>TDB2_DATA/ENDPOINT</code>: Relevant only if the SPARQL microservice is enabled. Defines the local path to a TDB2 database and the name of the endpoint exposed to it (e.g., /uce-ontology).</sup> </li> <li><sup><code>IMPORTER_THREADS</code>: The number of parallel threads used by the Importer to process corpus imports. (Typical values are 4, 8, or 16, depending on your system setup.)</sup> </li> <li><sup><code>POSTGRESQL_CONFIG</code>: A configuration file that defines PostgreSQL\u2019s resource allocation (e.g., memory, cores, etc.). Since configuration can be extensive, a helpful tool is pgtune.</sup> </li> <li><sup><code>KC_REALM_IMPORT_PATH</code>: Relevant only if the Keycloak microservice is enabled. This file contains a predefined realm for Keycloak, providing default configurations for UCE to work out of the box with Keycloak authentication. It is not recommended to edit this JSON file directly; instead, use the Keycloak web portal UI.</sup> </li> <li><sup><code>KC_ADMIN_USERNAME/PW</code>: Relevant only if the Keycloak microservice is enabled. Specifies the admin username and password for the Keycloak web UI. You will be prompted to log in when starting the Keycloak server. Change these values in production!</sup></li> </ul> <p>Start the relevant docker containers:</p> <pre><code>docker-compose up --build uce-postgresql-db uce-web\n</code></pre> <p>The web instance, by default, is reachable under: <code>http://localhost:8008</code>. As a User, you can now proceed to import data!</p> <p>Problems?</p> <p>If the webportal container isn't working, it most likely can't connect to the database. In that case, you can check the connection strings within the <code>common-release.conf</code> file (the <code>common.conf</code> file that is used for the docker build) in the source code. The ports and connection urls must match those in the <code>docker-compose</code>.</p>"},{"location":"getting-started/webportal/#developer-setup","title":"Developer Setup","text":"<p>Developer Code</p> <p>Please refer to the Developer Code for details on how to correctly develop UCE.</p> <p>Clone the UCE repo and switch to the <code>develop</code> branch:</p> <pre><code>git clone https://github.com/texttechnologylab/UCE.git\ngit fetch --all\ngit checkout origin develop\n</code></pre> <p>Before opening the repo in an IDE of your choice (but for this documentation, we will always refer to IntelliJ), we have to setup the database first.</p>"},{"location":"getting-started/webportal/#database","title":"Database","text":"<p>To set up the PostgreSQL database, you can either use a Docker image (refer to User Setup to do so via the compose file or install the database locally. When installing it locally, you must install the <code>pgvector</code> extension, as we configure PostgreSQL to work with high-dimensional embedding vectors for UCE. This requires a manual but simple installation. Additionally, the PostGIS extension is needed, as UCE may need to execute geographic queries. To avoid this manual installation, we recommend using the docker image within UCE's own <code>docker-compose</code> file instead.</p> <p>Local Installation</p> <p>If installed locally, you also need to manually create a database called <code>uce</code>, with the owner set to <code>postgres</code> and the default password set to <code>1234</code>. If you adjust any of these parameters, you must also update the corresponding values in the source code's <code>common.conf</code>.</p> <p>Respectively, when running the container from the official image (and not UCE's docker-compose), pass these parameters into the container: </p><pre><code>POSTGRES_DB: uce\nPOSTGRES_USER: postgres\nPOSTGRES_PASSWORD: 1234\n</code></pre><p></p>"},{"location":"getting-started/webportal/#web","title":"Web","text":"<p>If the PostgreSQL DB is running, start by opening IntelliJ from within the <code>uce.portal</code> folder (not the root of the repo) and setting up the IDE for the web portal:</p> <p>Setup</p> <ul> <li>Add a new <code>Application</code> configuration  </li> <li>UCE is being developed in Java 21 </li> </ul> Missing JDK Version? <p>(If the Java 21 SDK is missing, you need to install it. IntelliJ offers a build-in way for that through <code>Menu</code> -&gt; <code>Project Structure</code> -&gt; <code>Project</code> -&gt; Open the <code>SDK dropdown</code> -&gt; <code>Download JDK</code> and download any 21 version.) </p> <ul> <li>Set <code>-cp web</code> </li> <li>Main class: <code>org.texttechnologylab.App</code> </li> <li>Program arguments can be left empty for now. For a list of potential CLI arguments, refer to the documentation.</li> <li>Maven should automatically download and index the dependencies. If, for some reason, it does not, you can force an update via <code>mvn clean install -U</code> (in IntelliJ, open <code>Execute Maven Goal</code>, then enter the command).</li> </ul> <p>Now start the web portal. The default URL is <code>http://localhost:4567</code> and, if done correctly, the portal will appear with no corpora available. We will now set up the Corpus-Importer to import corpora.</p> <p>Java Version Error?</p> <p>Make sure that IntelliJ's Java compiler is also set to match the target bytecode version 21. Otherwise, startup will result in an error. You can check this via <code>Settings</code> \u2192 <code>Build, Execution, Deployment</code> \u2192 <code>Compiler</code> \u2192 <code>Java Compiler</code>.</p>"},{"location":"getting-started/webportal/#cli-arguments","title":"CLI Arguments","text":""},{"location":"getting-started/webportal/#web-portal","title":"Web Portal","text":"Argument Description <code>-cf</code> <code>--configFile</code> The local path to the uceConfig.json. If started through a Docker container, remember to first mount the local path and then map the <code>-cf</code> path to the mounted Docker path. <code>-cj</code> <code>--configJson</code> Pass in the contents of a <code>uceConfig.json</code> as a single json string. <code>-lex</code> Force the full lexicalization of all annotations upon UCE start. Use this, if you feel like the <code>Lexicon</code> page within UCE wasn't initialized properly."},{"location":"projects/","title":"Projects","text":"Projects <p>UCE is applied across a variety of projects, some of which are outlined in this section, as they are open-source and/or publicly available, while some are not.</p> <p>Depending on the domain and subject matter - ranging from biology to legal contexts - the NLP pipeline, and consequently the UCE portal, are dynamically adapted.</p> <p>If you would like to propose a new project using UCE and/or our NLP tools, feel free to contact us!</p>"},{"location":"projects/biofid/","title":"BIOfid","text":"<p>The BIOfid Specialised Information Service Biodiversity Research project uses NLP-centric annotations for historical and contemporary biodiversity literature, which is then made accessible via UCE.</p>"},{"location":"projects/biofid/#what-is-biofid","title":"What is BIOfid?","text":"<p>BIOfid is a German specialised information service that provides access to current and historical biodiversity literature and develops technology to mobilise data buried in printed sources. It is a cooperation between University Library J.C. Senckenberg (Frankfurt), Senckenberg Gesellschaft f\u00fcr Naturforschung, and the Text Technology Lab (Goethe University Frankfurt), funded by the DFG. BIOfid runs a text-mining pilot to extract structured biodiversity knowledge from literature-initially focused on birds, butterflies, and vascular plants-and prepares semantic search over the mobilised data through UCE.</p>"},{"location":"projects/biofid/#nlp-pipeline","title":"NLP Pipeline","text":"<p>The current (10/2025) NLP pipeline (implemented in DUUI) used for the BIOfid project goes as follows:</p> <pre><code>graph LR\n    A{Corpus} --&gt; B[spaCy]\n\n    subgraph DUUI[\"DUUI NLP Pipeline\"]\n        B --&gt; C[HeidelTimeX]\n        C --&gt; D[GeoNames]\n        D --&gt; E[Gazetteer]\n        E --&gt; F[GNFinder]\n    end\n\n    F --&gt; G{UCE}</code></pre> Layer Description Corpus Historical biodiversity literature is digitised and converted via OCR (ABBYY format) to create a machine-readable corpus. The OCR output retains structural metadata such as pages, paragraphs, blocks, and lines, forming the foundational document layout for subsequent NLP annotations. These structures are parsed and aligned during corpus import in DUUI to preserve document hierarchy. spaCy Provides the core linguistic preprocessing pipeline, including tokenization, part-of-speech tagging, lemmatization, sentence segmentation, and named entity recognition (NER). Built on neural transition-based parsing and newer CNN/transformer embeddings, spaCy efficiently handles both modern and historical German and English texts. Its modular pipeline allows integration with custom models or external components like HeidelTimeX and Gazetteer. \ud83d\udd17 spaCy Documentation HeidelTimeX Performs temporal expression recognition and normalization, essentially annotating mentions of time and periods. HeidelTimeX is a multilingual, rule-based temporal tagger derived from the original HeidelTime system but extended and additional domain adaptations for the German language. It uses regular expressions and handcrafted pattern rules for temporal phrase detection, coupled with a normalization module that converts relative or vague time expressions into ISO-TimeML standard formats (e.g., <code>2024-10-21</code> or <code>P2Y</code>). \ud83d\udd17 HeidelTimeX Paper GeoNames Custom DUUI component that performs geographical entity resolution by linking location mentions (identified by spaCy's NER) to entries in the GeoNames database. Uses fuzzy string matching and heuristic normalization to retrieve metadata such as coordinates, population, and administrative hierarchy. This enriches the corpus with geospatial attributes that can be visualized or filtered in UCE. Gazetteer Performs taxon name recognition and linking using a hybrid of dictionary-based, morphological, and regular-expression methods. Implemented in Rust, the <code>gazetteer-rs</code> library is optimized for speed and large-scale corpus processing. It loads curated taxonomic name dictionaries and employs trie-based lookups and fuzzy matching to identify scientific names within running text, especially abbreviations, and can associate recognized taxa with unique identifiers (e.g., GBIF or BioFID ontology IDs). Its integration in DUUI allows taxon annotations. GNFinder Executes scientific name detection and normalization based on natural language heuristics and biodiversity nomenclature rules. GNFinder tokenizes text and applies a combination of regular expressions, Bayesian classifiers, and name verification algorithms to identify valid Latin binomials (e.g., Homo sapiens). It cross-references results with biodiversity databases (e.g., Catalogue of Life, GBIF, WoRMS) to verify and canonicalize them. GNFinder can disambiguate author abbreviations, hybrid notations, and extinct species marks, returning both normalized names and their taxonomic identifiers. <p>Additional Annotations</p> <p>Within the BIOfid project, additional tools and NLP tasks have been developed and annotated. The list above represents only the annotations currently available in the UCE BIOfid portal. Further annotations, such as Semantic Role Labeling and more human-centered tools based on newly created ontologies have been created and will be integrated into the UCE portal soon.</p>"},{"location":"projects/biofid/#how-this-translates-into-uce","title":"How this translates into UCE","text":"<p>UCE can leverage the various annotations performed on the data to build custom features around them. For example:</p> <ul> <li> <p>Geographic queries:   UCE uses GeoNames annotations to enable location-based searches. Users can select a point on a map and define a radius, which highlights all pages and documents associated with locations within that area. Additionally, users can filter by GeoNames classes and codes, allowing queries such as <code>Frog &amp; LOC::T</code>, which searches for mentions of frogs in association with any type of mountain, hill, rock, etc.</p> </li> <li> <p>Temporal queries:   Similarly, time annotations allow users to filter or search within specific dates, ranges, or even seasons (e.g., \"winter\") using UCE\u2019s boolean search logic (AND, OR, NOT, and more).</p> </li> <li> <p>Taxonomic queries:   Taxa annotated by Gazetteer and GNFinder enable filtering based on taxonomic rank (class, family, kingdom, phylum, etc.). These can also be combined with spatial and temporal filters. Furthermore, this allows mapping results onto an interactive 2D map, where detected taxa are grouped by their location and time.</p> </li> </ul> <p>These are just a few examples of how UCE uses annotations for enrichment, search, and visualization-and many more features are available.</p>"},{"location":"projects/biofid/#uce-and-ontologies","title":"UCE and Ontologies","text":"<p>Within the BIOfid project, UCE leverages the SPARQL service to integrate and utilize custom ontologies developed by biologists. The following outlines all operations performed with and on these ontologies within UCE. All corresponding queries can be found in UCE\u2019s JenaSparqlService.java.</p> Operation Description Importing During import, UCE reads the UIMA files containing taxa annotated by Gazetteer and GNFinder. Each annotation includes an identifier - either a BIOfid-specific URL or a GBIF identifier. UCE uses this identifier to query the SPARQL ontology, retrieve all vernacular and alternative names for the taxon, and store them in UCE\u2019s database. Search Enrichment When enrichment mode or pro-mode is activated, UCE checks each token against its internal taxa database (previously populated during import with alternative names). If a token is not found, it is not considered a searchable taxon within UCE - though it may still be a taxon outside the system. If it is found, UCE retrieves the unique identifier stored in its database and queries the SPARQL ontology for vernacular names, synonyms, and lower taxonomic ranks (e.g., subspecies, varietas, variety, forma, form). These enriched tokens are then added to the search with an OR concatenation. It is being considered to remove this additional SPARQL query during search by expanding enrichment during import, so only the UCE database needs to be queried. Taxon Commands When a user enters a taxon command (e.g., <code>G::Bellis</code>), the SPARQL ontology is queried for any entry with the taxonomic rank Genus and the scientific name Bellis. This returns a unique identifier, which is then used to query the SPARQL database again to retrieve all species where the genus equals that ID. As in the search enrichment, alternative and synonym names are also fetched for each found species. Layered Search In layered search mode, SPARQL is again used to resolve taxon commands. However, document and page filtering is performed exclusively on the taxa already stored in UCE\u2019s database (imported annotations), rather than via string-based search. This method is not limited by a maximum number of taxa - unlike full-text search - since it operates on structured annotation data instead of complex text queries."},{"location":"projects/biofid/#jenasparqlservicejava","title":"JenaSparqlService.java","text":"<p>Below, you can find the current (10/2025) queries made to the SPARQL service for this specific BIOfid ontology and project.</p> Method Return Type Description <code>queryBiofidTaxon(String biofidUrl)</code> <code>List&lt;BiofidTaxon&gt;</code>          Given a BIOfid taxon URL, retrieves all RDF triples for the subject and converts them into <code>BiofidTaxon</code> objects.          Query <pre><code>SELECT * WHERE {\n  &lt;{SUB}&gt; ?pred ?obj .\n}\nLIMIT 100</code></pre> <code>queryBySubject(String sub)</code> <code>List&lt;RDFNodeDto&gt;</code>          Returns all RDF triples (subject, predicate, object) where the given URI is the subject. Filters out generic RDF/OWL predicates.          Query <pre><code>SELECT * WHERE {\n  &lt;{SUB}&gt; ?pred ?obj .\n}\nLIMIT 100</code></pre> <code>getSpeciesIdsOfUpperRank(String rank, String name, int limit)</code> <code>List&lt;String&gt;</code>          Fetches all species identifiers under a specified higher taxonomic rank (e.g., Genus \u2192 Species). Internally uses <code>getIdsOfTaxonRank()</code> and <code>getSpeciesOfRank()</code>.        <code>getIdsOfTaxonRank(String rank, String name)</code> <code>List&lt;String&gt;</code>          Returns identifiers (URIs) of all taxa that match a given rank and scientific name. Used to locate parent taxa.          Query <pre><code>SELECT distinct ?subject WHERE {\n  ?subject &lt;http://rs.tdwg.org/dwc/terms/taxonRank&gt; \"{RANK}\"^^&lt;xsd:string&gt; .\n  ?subject &lt;http://rs.tdwg.org/dwc/terms/cleanedScientificName&gt; \"{NAME}\" .\n}\nLIMIT 1</code></pre> <code>getSpeciesOfRank(String rankName, List&lt;String&gt; ids, int limit)</code> <code>List&lt;String&gt;</code>          Given rank identifiers, retrieves all species belonging to them, limited by the specified number.          Query <pre><code>SELECT DISTINCT ?subject WHERE {\n  ?subject &lt;http://rs.tdwg.org/dwc/terms/taxonRank&gt; \"species\"^^&lt;xsd:string&gt; .\n  ?subject &lt;http://rs.tdwg.org/dwc/terms/{RANK}&gt; ?rank .\n  VALUES ?rank {\n    {IDS}\n  }\n}\nLIMIT {LIMIT}</code></pre> <code>getPossibleSynonymIdsOfTaxon(List&lt;String&gt; biofidUrls)</code> <code>List&lt;String&gt;</code>          Returns all URIs of taxa marked as synonyms of the provided BIOfid taxon URIs (via <code>dwc:acceptedNameUsageID</code>).          Query <pre><code>PREFIX dwc: &lt;http://rs.tdwg.org/dwc/terms/&gt;\nSELECT ?subject\nWHERE {\n  ?subject dwc:acceptedNameUsageID &lt;%s&gt; .\n  ?subject dwc:taxonomicStatus ?status .\n  FILTER(lcase(str(?status)) = \"synonym\")\n}</code></pre> <code>getSubordinateTaxonIds(List&lt;String&gt; biofidUrls)</code> <code>List&lt;String&gt;</code>          Returns all subordinate taxa (e.g., subspecies, varietas, variety, forma, form) for the given BIOfid URIs.          Query <pre><code>PREFIX dwc: &lt;http://rs.tdwg.org/dwc/terms/&gt;\nSELECT ?subject ?object\nWHERE {\n  ?subject dwc:parentNameUsageID &lt;%s&gt; .\n  ?subject dwc:taxonRank ?object .\n  FILTER(lcase(str(?object)) IN (\"subspecies\", \"varietas\", \"variety\", \"forma\", \"form\"))\n}</code></pre> <code>getAlternativeNamesOfTaxons(List&lt;String&gt; biofidIds)</code> <code>List&lt;String&gt;</code>          Combines synonym and subordinate taxon lookups to retrieve all vernacular and cleaned scientific names for the given taxa.          Query <pre><code>SELECT ?subject ?predicate ?object\nWHERE {\n  VALUES ?subject { {BIOFID_IDS} }\n  ?subject ?predicate ?object .\n  FILTER(?predicate IN (\n    &lt;http://rs.tdwg.org/dwc/terms/vernacularName&gt;,\n    &lt;http://rs.tdwg.org/dwc/terms/cleanedScientificName&gt;\n  ))\n}</code></pre> <code>biofidIdUrlToGbifTaxonId(String potentialBiofidId)</code> <code>long</code>          Converts a BIOfid taxon URL into its GBIF taxon ID (last numeric segment). Returns <code>-1</code> if none found.          Query <pre><code>SELECT ?predicate ?object\nWHERE {\n  &lt;{BIOFID_URL_ID}&gt; &lt;http://rs.tdwg.org/dwc/terms/taxonID&gt; ?object .\n}</code></pre>"}]}